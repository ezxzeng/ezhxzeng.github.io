<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Emily ZhiXuan Zeng </title> <meta name="author" content="Emily Zhixuan Zeng"> <meta name="description" content="I am Emily Zhixuan Zeng. PhD student focusing on Machine Learning and Computer Vision, University of Waterloo. "> <meta name="keywords" content="machine-learning, computer-vision, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;EZZ&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.ezxzeng.com/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Emily ZhiXuan Zeng </h1> <p class="desc"><a href="#">University of Waterloo</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile-480.webp 480w,/assets/img/profile-800.webp 800w,/assets/img/profile-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/profile.jpg?0a378eb14adf0b2a51c6a3961562dae9" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am Emily Zhixuan Zeng.</p> <p>PhD student focusing on Machine Learning and Computer Vision,</p> <p>University of Waterloo.</p> </div> <h2> <a href="/work/" style="color: inherit;">Work experience</a> </h2> <div class="projects-list"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Nvidia_logo-480.webp 480w,/assets/img/Nvidia_logo-800.webp 800w,/assets/img/Nvidia_logo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/Nvidia_logo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="assets/img/Nvidia_logo.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nvidia-lane" class="col-sm-8"> <div class="date_string"> <em>May-August 2021</em> </div> <div class="title">NVIDIA - Autonomous vehicle</div> <div class="description"> <p>Incorporated synthetic data for training lane detection model to target challenging scenarios</p> </div> <div class="links"> <a href="https://www.nvidia.com/en-us/self-driving-cars/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Company Website</a> </div> </div> <div class="related_publications hidden"> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Nvidia_logo-480.webp 480w,/assets/img/Nvidia_logo-800.webp 800w,/assets/img/Nvidia_logo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/Nvidia_logo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="assets/img/Nvidia_logo.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nvidia-lights" class="col-sm-8"> <div class="date_string"> <em>May-August 2020</em> </div> <div class="title">NVIDIA - Autonomous vehicle</div> <div class="description"> <p>Time series light signal detection for autonomous vehicles</p> </div> <div class="links"> <a href="https://www.nvidia.com/en-us/self-driving-cars/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Company Website</a> </div> </div> <div class="related_publications hidden"> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Miovision_logo-480.webp 480w,/assets/img/Miovision_logo-800.webp 800w,/assets/img/Miovision_logo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/Miovision_logo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="/assets/img/Miovision_logo.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="miovision" class="col-sm-8"> <div class="date_string"> <em>Sept-Dec 2019</em> </div> <div class="title">Miovision</div> <div class="description"> <p>Automating data ingest for maximum training improvement</p> </div> <div class="links"> <a href="https://miovision.com/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Company Website</a> </div> </div> <div class="related_publications hidden"> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/synapse-480.webp 480w,/assets/img/synapse-800.webp 800w,/assets/img/synapse-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/synapse.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="/assets/img/synapse.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="synapse" class="col-sm-8"> <div class="date_string"> <em>Jan-April 2019</em> </div> <div class="title">Synapse Technology</div> <div class="description"> <p>Developed and analyzed CNN models for detecting threats from x-ray scans</p> </div> <div class="links"> <a href="https://www.linkedin.com/company/synapse-technology-corporation/about/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">About Company</a> </div> </div> <div class="related_publications hidden"> </div> </div> </li> </ol> </div> <h2> <a href="/projects/" style="color: inherit;">Projects</a> </h2> <div class="projects-list"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chicken_burger-480.webp 480w,/assets/img/publication_preview/chicken_burger-800.webp 800w,/assets/img/publication_preview/chicken_burger-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/chicken_burger.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="/assets/img/publication_preview/chicken_burger.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="explaining_diffusion" class="col-sm-8"> <div class="date_string"> <em></em> </div> <div class="title">Explaining Diffusion</div> <div class="description"> <p>[Current, in progress] understanding relationships in concepts learned by image generation models</p> </div> <div class="links"> <a class="related_publications btn btn-sm z-depth-0" role="button">related publications</a> </div> </div> <div class="related_publications hidden"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chicken_burger-480.webp 480w,/assets/img/publication_preview/chicken_burger-800.webp 800w,/assets/img/publication_preview/chicken_burger-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/chicken_burger.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chicken_burger.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zeng2024understanding" class="col-sm-8"> <div class="title">Understanding the Limitations of Diffusion Concept Algebra Through Food</div> <div class="author"> <em>E Zhixuan Zeng</em>, Yuhao Chen , and Alexander Wong </div> <div class="periodical"> <em>arXiv preprint arXiv:2406.03582</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2406.03582" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Image generation techniques, particularly latent diffusion models, have exploded in popularity in recent years. Many techniques have been developed to manipulate and clarify the semantic concepts these large-scale models learn, offering crucial insights into biases and concept relationships. However, these techniques are often only validated in conventional realms of human or animal faces and artistic style transitions. The food domain offers unique challenges through complex compositions and regional biases, which can shed light on the limitations and opportunities within existing methods. Through the lens of food imagery, we analyze both qualitative and quantitative patterns within a concept traversal technique. We reveal measurable insights into the model’s ability to capture and represent the nuances of culinary diversity, while also identifying areas where the model’s biases and limitations emerge.</p> </div> </div> </div> </li></ol> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/4sfJMv9-480.webp 480w,https://i.imgur.com/4sfJMv9-800.webp 800w,https://i.imgur.com/4sfJMv9-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="https://i.imgur.com/4sfJMv9.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="https://i.imgur.com/4sfJMv9.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="explaining_explainability" class="col-sm-8"> <div class="date_string"> <em></em> </div> <div class="title">Second order XAI</div> <div class="description"> <p>Explaining Explainability: Towards Deeper Actionable Insights into Deep Learning through Second-order Explainability</p> </div> <div class="links"> <a class="related_publications btn btn-sm z-depth-0" role="button">related publications</a> </div> </div> <div class="related_publications hidden"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/4sfJMv9-480.webp 480w,https://i.imgur.com/4sfJMv9-800.webp 800w,https://i.imgur.com/4sfJMv9-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="https://i.imgur.com/4sfJMv9.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="https://i.imgur.com/4sfJMv9.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zeng2023explaining_xai" class="col-sm-8"> <div class="title">Explaining Explainability: Towards Deeper Actionable Insights into Deep Learning through Second-order Explainability</div> <div class="author"> <em>E Zhixuan Zeng*</em>, Hayden Gunraj* , Sheldon Fernandez , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexander Wong' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In XAI4CV workshop</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2306.08780" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Explainability plays a crucial role in providing a more comprehensive understanding of deep learning models’ behaviour. This allows for thorough validation of the model’s performance, ensuring that its decisions are based on relevant visual indicators and not biased toward irrelevant patterns existing in training data. However, existing methods provide only instance-level explainability, which requires manual analysis of each sample. Such manual review is time-consuming and prone to human biases. To address this issue, the concept of second-order explainable AI (SOXAI) was recently proposed to extend explainable AI (XAI) from the instance level to the dataset level. SOXAI automates the analysis of the connections between quantitative explanations and dataset biases by identifying prevalent concepts. In this work, we explore the use of this higher-level interpretation of a deep neural network’s behaviour to allows us to "explain the explainability" for actionable insights. Specifically, we demonstrate for the first time, via example classification and segmentation cases, that eliminating irrelevant concepts from the training set based on actionable insights from SOXAI can enhance a model’s performance.</p> </div> </div> </div> </li></ol> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/drop_item-480.webp 480w,/assets/img/publication_preview/drop_item-800.webp 800w,/assets/img/publication_preview/drop_item-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/drop_item.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="/assets/img/publication_preview/drop_item.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="metagraspnet" class="col-sm-8"> <div class="date_string"> <em></em> </div> <div class="title">MetaGraspNet</div> <div class="description"> <p>Robotic Grasping Dataset and object pose estimation via superquadrics</p> </div> <div class="links"> <a class="related_publications btn btn-sm z-depth-0" role="button">related publications</a> </div> </div> <div class="related_publications hidden"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/shapeshift-480.webp 480w,/assets/img/publication_preview/shapeshift-800.webp 800w,/assets/img/publication_preview/shapeshift-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/shapeshift.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="shapeshift.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zeng2023shapeshift" class="col-sm-8"> <div class="title">ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping</div> <div class="author"> <em>E Zhixuan Zeng</em>, Yuhao Chen , and Alexander Wong </div> <div class="periodical"> <em>In WICV workshop</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2304.04861" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Object pose estimation is a critical task in robotics for precise object manipulation. However, current techniques heavily rely on a reference 3D object, limiting their generalizability and making it expensive to expand to new object categories. Direct pose predictions also provide limited information for robotic grasping without referencing the 3D model. Keypoint-based methods offer intrinsic descriptiveness without relying on an exact 3D model, but they may lack consistency and accuracy. To address these challenges, this paper proposes ShapeShift, a superquadric-based framework for object pose estimation that predicts the object’s pose relative to a primitive shape which is fitted to the object. The proposed framework offers intrinsic descriptiveness and the ability to generalize to arbitrary geometric shapes beyond the training set.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmrnet-480.webp 480w,/assets/img/publication_preview/mmrnet-800.webp 800w,/assets/img/publication_preview/mmrnet-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmrnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmrnet.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2023mmrnet" class="col-sm-8"> <div class="title">MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy</div> <div class="author"> Yuhao Chen , Hayden Gunraj , <em>E Zhixuan Zeng</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Robbie Meyer, Maximilian Gilles, Alexander Wong' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2210.10842" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to address sensor failure issues during deployment. In particular, we realize the multimodal redundancy framework with a gate fusion module and dynamic ensemble learning. Finally, we present a new label-free multi-modal consistency (MC) score that utilizes the output from all modalities to measure the overall system output reliability and uncertainty. Through experiments, we demonstrate that in an event of missing modality, our system provides a much more reliable performance compared to baseline models. We also demonstrate that our MC score is a more reliability indicator for outputs during inference time compared to the model generated confidence scores that are often over-confident.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/drop_item-480.webp 480w,/assets/img/publication_preview/drop_item-800.webp 800w,/assets/img/publication_preview/drop_item-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/drop_item.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="drop_item.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2022metagraspnet" class="col-sm-8"> <div class="title">MetaGraspNet: A Large-Scale Benchmark Dataset for Vision-driven Robotic Grasping via Physics-based Metaverse Synthesis</div> <div class="author"> Yuhao Chen , Maximilian Gilles , <em>E Zhixuan Zeng</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexander Wong' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2022 IEEE CASE</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awareded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2208.03963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Finalist</p> </div> <div class="abstract hidden"> <p>Autonomous bin picking poses significant challenges to vision-driven robotic systems given the complexity of the problem, ranging from various sensor modalities, to highly entangled object layouts, to diverse item properties and gripper types. Existing methods often address the problem from one perspective. Diverse items and complex bin scenes require diverse picking strategies together with advanced reasoning. As such, to build robust and effective machine-learning algorithms for solving this complex task requires significant amounts of comprehensive and high quality data. Collecting such data in real world would be too expensive and time prohibitive and therefore intractable from a scalability perspective. To tackle this big, diverse data problem, we take inspiration from the recent rise in the concept of metaverses, and introduce MetaGraspNet, a large-scale photo-realistic bin picking dataset constructed via physics-based metaverse synthesis. The proposed dataset contains 217k RGBD images across 82 different article types, with full annotations for object detection, amodal perception, keypoint detection, manipulation order and ambidextrous grasp labels for a parallel-jaw and vacuum gripper. We also provide a real dataset consisting of over 2.3k fully annotated high-quality RGBD images, divided into 5 levels of difficulties and an unseen object set to evaluate different object and layout properties. Finally, we conduct extensive experiments showing that our proposed vacuum seal model and synthetic dataset achieves state-of-the-art performance and generalizes to real world use-cases.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zeng2022keypoints" class="col-sm-8"> <div class="title">Investigating Use of Keypoints for Object Pose Recognition</div> <div class="author"> <em>E Zhixuan Zeng</em>, Yuhao Chen , and Alexander Wong </div> <div class="periodical"> <em>In Journal of Computational Vision and Imaging Systems</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openjournals.uwaterloo.ca/index.php/vsl/article/view/5382" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Object pose detection is a task that is highly useful for a variety of object manipulation tasks such as robotic grasping and tool handling. Perspective-n-Point matching between keypoints on the objects offers a way to perform pose estimation where the keypoints also provide inherent object information, such as corner locations and object part sections, without the need to reference a separate 3D model. Existing works focus on scenes with little occlusion and limited object categories. In this study, we demonstrate the feasibility of a pose estimation network based on detecting semantically important keypoints on the MetagraspNet dataset which contains heavy occlusion and greater scene complexity. We further discuss various challenges in using semantically important keypoints as a way to perform object pose estimation. These challenges include maintaining consistent keypoint definition, as well as dealing with heavy occlusion and similar visual features. </p> </div> </div> </div> </li> </ol> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/warp_example_base-480.webp 480w,/assets/img/publication_preview/warp_example_base-800.webp 800w,/assets/img/publication_preview/warp_example_base-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/warp_example_base.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="/assets/img/publication_preview/warp_example_base.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="covidnet" class="col-sm-8"> <div class="date_string"> <em></em> </div> <div class="title">COVID-Net US-X</div> <div class="description"> <p>Enhanced Deep Neural Network for Detection of COVID-19 Patient Cases from Convex Ultrasound Imaging Through Extended Linear-Convex Ultrasound Augmentation Learning</p> </div> <div class="links"> <a class="related_publications btn btn-sm z-depth-0" role="button">related publications</a> </div> </div> <div class="related_publications hidden"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/warp_example_base-480.webp 480w,/assets/img/publication_preview/warp_example_base-800.webp 800w,/assets/img/publication_preview/warp_example_base-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/warp_example_base.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="warp_example_base.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zeng2024covid" class="col-sm-8"> <div class="title">COVID-Net L2C-ULTRA: An Explainable Linear-Convex Ultrasound Augmentation Learning Framework to Improve COVID-19 Assessment and Monitoring</div> <div class="author"> <em>E Zhixuan Zeng</em>, Ashkan Ebadi , Adrian Florea , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexander Wong' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Sensors</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.mdpi.com/1424-8220/24/5/1664" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While no longer a public health emergency of international concern, COVID-19 remains an established and ongoing global health threat. As the global population continues to face significant negative impacts of the pandemic, there has been an increased usage of point-of-care ultrasound (POCUS) imaging as a low-cost, portable, and effective modality of choice in the COVID-19 clinical workflow. A major barrier to the widespread adoption of POCUS in the COVID-19 clinical workflow is the scarcity of expert clinicians who can interpret POCUS examinations, leading to considerable interest in artificial intelligence-driven clinical decision support systems to tackle this challenge. A major challenge to building deep neural networks for COVID-19 screening using POCUS is the heterogeneity in the types of probes used to capture ultrasound images (e.g., convex vs. linear probes), which can lead to very different visual appearances. In this study, we propose an analytic framework for COVID-19 assessment able to consume ultrasound images captured by linear and convex probes. We analyze the impact of leveraging extended linear-convex ultrasound augmentation learning on producing enhanced deep neural networks for COVID-19 assessment, where we conduct data augmentation on convex probe data alongside linear probe data that have been transformed to better resemble convex probe data. The proposed explainable framework, called COVID-Net L2C-ULTRA, employs an efficient deep columnar anti-aliased convolutional neural network designed via a machine-driven design exploration strategy. Our experimental results confirm that the proposed extended linear–convex ultrasound augmentation learning significantly increases performance, with a gain of 3.9% in test accuracy and 3.2% in AUC, 10.9% in recall, and 4.4% in precision. The proposed method also demonstrates a much more effective utilization of linear probe images through a 5.1% performance improvement in recall when such images are added to the training dataset, while all other methods show a decrease in recall when trained on the combined linear–convex dataset. We further verify the validity of the model by assessing what the network considers to be the critical regions of an image with our contribution clinician.</p> </div> </div> </div> </li></ol> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/autoread-system-cropped-480.webp 480w,/assets/img/autoread-system-cropped-800.webp 800w,/assets/img/autoread-system-cropped-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/autoread-system-cropped.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="/assets/img/autoread-system-cropped.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="autoread" class="col-sm-8"> <div class="date_string"> <em></em> </div> <div class="title">AutoRead</div> <div class="description"> <p>BASc Capstone Project. The dramatic automatic audiobook maker. Using the power of text to speach, we seek to generate suitably dramatic readings for fiction novels. Uses fully automated dataset generation for training a custom text to speach model.</p> </div> <div class="links"> <a href="https://autoread-fydp.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> <div class="related_publications hidden"> </div> </div> </li> </ol> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <br> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/drop_item-480.webp 480w,/assets/img/publication_preview/drop_item-800.webp 800w,/assets/img/publication_preview/drop_item-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/drop_item.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="drop_item.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2022metagraspnet" class="col-sm-8"> <div class="title">MetaGraspNet: A Large-Scale Benchmark Dataset for Vision-driven Robotic Grasping via Physics-based Metaverse Synthesis</div> <div class="author"> Yuhao Chen , Maximilian Gilles , <em>E Zhixuan Zeng</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexander Wong' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2022 IEEE CASE</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awareded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2208.03963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Finalist</p> </div> <div class="abstract hidden"> <p>Autonomous bin picking poses significant challenges to vision-driven robotic systems given the complexity of the problem, ranging from various sensor modalities, to highly entangled object layouts, to diverse item properties and gripper types. Existing methods often address the problem from one perspective. Diverse items and complex bin scenes require diverse picking strategies together with advanced reasoning. As such, to build robust and effective machine-learning algorithms for solving this complex task requires significant amounts of comprehensive and high quality data. Collecting such data in real world would be too expensive and time prohibitive and therefore intractable from a scalability perspective. To tackle this big, diverse data problem, we take inspiration from the recent rise in the concept of metaverses, and introduce MetaGraspNet, a large-scale photo-realistic bin picking dataset constructed via physics-based metaverse synthesis. The proposed dataset contains 217k RGBD images across 82 different article types, with full annotations for object detection, amodal perception, keypoint detection, manipulation order and ambidextrous grasp labels for a parallel-jaw and vacuum gripper. We also provide a real dataset consisting of over 2.3k fully annotated high-quality RGBD images, divided into 5 levels of difficulties and an unseen object set to evaluate different object and layout properties. Finally, we conduct extensive experiments showing that our proposed vacuum seal model and synthetic dataset achieves state-of-the-art performance and generalizes to real world use-cases.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/shapeshift-480.webp 480w,/assets/img/publication_preview/shapeshift-800.webp 800w,/assets/img/publication_preview/shapeshift-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/shapeshift.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="shapeshift.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zeng2023shapeshift" class="col-sm-8"> <div class="title">ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping</div> <div class="author"> <em>E Zhixuan Zeng</em>, Yuhao Chen , and Alexander Wong </div> <div class="periodical"> <em>In WICV workshop</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2304.04861" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Object pose estimation is a critical task in robotics for precise object manipulation. However, current techniques heavily rely on a reference 3D object, limiting their generalizability and making it expensive to expand to new object categories. Direct pose predictions also provide limited information for robotic grasping without referencing the 3D model. Keypoint-based methods offer intrinsic descriptiveness without relying on an exact 3D model, but they may lack consistency and accuracy. To address these challenges, this paper proposes ShapeShift, a superquadric-based framework for object pose estimation that predicts the object’s pose relative to a primitive shape which is fitted to the object. The proposed framework offers intrinsic descriptiveness and the ability to generalize to arbitrary geometric shapes beyond the training set.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/4sfJMv9-480.webp 480w,https://i.imgur.com/4sfJMv9-800.webp 800w,https://i.imgur.com/4sfJMv9-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="https://i.imgur.com/4sfJMv9.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="https://i.imgur.com/4sfJMv9.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zeng2023explaining_xai" class="col-sm-8"> <div class="title">Explaining Explainability: Towards Deeper Actionable Insights into Deep Learning through Second-order Explainability</div> <div class="author"> <em>E Zhixuan Zeng*</em>, Hayden Gunraj* , Sheldon Fernandez , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexander Wong' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In XAI4CV workshop</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2306.08780" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Explainability plays a crucial role in providing a more comprehensive understanding of deep learning models’ behaviour. This allows for thorough validation of the model’s performance, ensuring that its decisions are based on relevant visual indicators and not biased toward irrelevant patterns existing in training data. However, existing methods provide only instance-level explainability, which requires manual analysis of each sample. Such manual review is time-consuming and prone to human biases. To address this issue, the concept of second-order explainable AI (SOXAI) was recently proposed to extend explainable AI (XAI) from the instance level to the dataset level. SOXAI automates the analysis of the connections between quantitative explanations and dataset biases by identifying prevalent concepts. In this work, we explore the use of this higher-level interpretation of a deep neural network’s behaviour to allows us to "explain the explainability" for actionable insights. Specifically, we demonstrate for the first time, via example classification and segmentation cases, that eliminating irrelevant concepts from the training set based on actionable insights from SOXAI can enhance a model’s performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmrnet-480.webp 480w,/assets/img/publication_preview/mmrnet-800.webp 800w,/assets/img/publication_preview/mmrnet-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmrnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmrnet.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2023mmrnet" class="col-sm-8"> <div class="title">MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy</div> <div class="author"> Yuhao Chen , Hayden Gunraj , <em>E Zhixuan Zeng</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Robbie Meyer, Maximilian Gilles, Alexander Wong' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2210.10842" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to address sensor failure issues during deployment. In particular, we realize the multimodal redundancy framework with a gate fusion module and dynamic ensemble learning. Finally, we present a new label-free multi-modal consistency (MC) score that utilizes the output from all modalities to measure the overall system output reliability and uncertainty. Through experiments, we demonstrate that in an event of missing modality, our system provides a much more reliable performance compared to baseline models. We also demonstrate that our MC score is a more reliability indicator for outputs during inference time compared to the model generated confidence scores that are often over-confident.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%65%7A%7A%65%6E%67@%75%77%61%74%65%72%6C%6F%6F.%63%61" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=mORYsEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/ezxzeng" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Emily Zhixuan Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 09, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?327dbdbc0cc890fe43e9cc3b3e4fb684"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>